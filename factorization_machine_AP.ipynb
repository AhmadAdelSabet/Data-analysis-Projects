{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AhmadAdelSabet/AhmadAdelSabet/blob/master/factorization_machine_AP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DSTA Lab 10_b\n",
        "This is an implementation of a factorization machine for binary classification. The notebook is downloaded from a GitHub repository maintained by MingYu (Ethen) Liu. For brevity, I removed first few cells from the original notebook. The link is as follows: \n",
        "https://nbviewer.org/github/ethen8181/machine-learning/blob/master/recsys/factorization_machine/factorization_machine.ipynb \n",
        "\n",
        "You need to download a text dataset (filename is sms.tsv) that contains text messages labelled as \"ham\" or \"spam\". "
      ],
      "metadata": {
        "id": "QrjKzdmkOvp8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "toc": "true",
        "id": "Goq9VzKjDEcY"
      },
      "source": [
        "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
        "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Factorization-Machine-(FM)\" data-toc-modified-id=\"Factorization-Machine-(FM)-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Factorization Machine (FM)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Implementation\" data-toc-modified-id=\"Implementation-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Implementation</a></span></li></ul></li><li><span><a href=\"#Reference\" data-toc-modified-id=\"Reference-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Reference</a></span></li></ul></div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHIlSPbaDEcc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blJF4mONDEcd"
      },
      "source": [
        "# Factorization Machine (FM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3aGOF6wDEcd"
      },
      "source": [
        "**Factorization Machine** type algorithms are a combination of linear regression and matrix factorization, the cool idea behind this type of algorithm is it aims model interactions between features (a.k.a attributes, explanatory variables) using factorized parameters. By doing so it has the ability to estimate all interactions between features even with extremely sparse data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hz7jn0niDEcd"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcXR43WMDEcd"
      },
      "source": [
        "Normally, when we think of linear regression, we would think of the following formula:\n",
        "\n",
        "\\begin{align}\n",
        "\\hat{y}(\\textbf{x}) = w_{0} + \\sum_{i=1}^{n} w_{i} x_{i}\n",
        "\\end{align}\n",
        "\n",
        "Where:\n",
        "\n",
        "- $w_0$ is the bias term, a.k.a intercept.\n",
        "- $w_i$ are weights corresponding to each feature vector $x_i$, here we assume we have $n$ total features.\n",
        "\n",
        "This formula's advantage is that it can computed in linear time, $O(n)$. The drawback, however, is that it does not handle feature interactions. To capture interactions, we could introduce a weight for each feature combination. This is sometimes referred to as a $2_{nd}$ ordered polynomial. The resulting model is shown below:\n",
        "\n",
        "\\begin{align}\n",
        "\\hat{y}(\\textbf{x}) = w_{0} + \\sum_{i=1}^{n} w_{i} x_{i} +  \\sum_{i=1}^n \\sum_{j=i+1}^n w_{ij} x_{i} x_{j}\n",
        "\\end{align}\n",
        "\n",
        "Compared to our previous model, this formulation has the advantages that it can capture feature interactions at least for two features at a time. But we have now ended up with a $O(n^2)$ complexity which means that to train the model, we now require a lot more time and memory. Another issue is that when we have categorical variables with high cardinality, after one-hot encoding them, we would end up with a lot of columns that are sparse, making it harder to actually capture their interactions (not enough data).\n",
        "\n",
        "To solve this complexity issue, Factorization Machines takes inspiration from [matrix factorization](http://nbviewer.jupyter.org/github/ethen8181/machine-learning/blob/master/recsys/1_ALSWR.ipynb), and models the feature interaction using latent factors. Every feature $f_i$ has a corresponding latent factor $v_i$, and two features' interactions are modelled as $\\langle \\textbf{v}_i, \\textbf{v}_{j} \\rangle$, where $\\langle \\cdot \\;,\\cdot \\rangle$ refers to the dot product of the two feature vector. If we assume its of size $k$ (this is a hyperparameter that we can tune). Then:\n",
        "\n",
        "\\begin{align}\n",
        "\\langle \\textbf{v}_i, \\textbf{v}_{j} \\rangle = \\sum_{f=1}^k v_{i,f} v_{j,f}\n",
        "\\end{align}\n",
        "\n",
        "This leads of our new equation:\n",
        "\n",
        "\\begin{align}\n",
        "\\hat{y}(\\textbf{x}) = w_{0} + \\sum_{i=1}^{n} w_{i} x_{i} + \\sum_{i=1}^{n} \\sum_{j=i+1}^n \\langle \\textbf{v}_i , \\textbf{v}_{j} \\rangle x_i x_{j}\n",
        "\\end{align}\n",
        "\n",
        "This is an improvement from our previous model (when we modeled each pair of interaction terms with weight $w_{ij}$) as the number of parameters is reduced from $n^2$ to $n \\times k$, since $k \\ll n$, which also helps mitigate overfitting issues. Using the naive way of formulating factorization machine results in a complexity of $O(kn^2)$, because all pairwise interactions have to be computed, but we can reformulate it to make it run in $O(kn)$.\n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "\\sum_{i=1}^n \\sum_{j=i+1}^n \\langle \\textbf{v}_i, \\textbf{v}_{j} \\rangle x_{i} x_{j}\n",
        "&= \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\langle \\textbf{v}_i, \\textbf{v}_{j} \\rangle x_{i} x_{j} - \\frac{1}{2} \\sum_{i=1}^n \\langle \\textbf{v}_i , \\textbf{v}_{i} \\rangle x_{i} x_{i}  \\\\\n",
        "&= \\frac{1}{2}\\left(\\sum_{i=1}^n \\sum_{j=1}^n \\sum_{f=1}^k v_{i,f} v_{j,f} x_{i} x_{j} \\right) - \\frac{1}{2}\\left( \\sum_{i=1}^n \\sum_{f=1}^k v_{i,f} v_{i,f} x_{i} x_{i} \\right) \\\\\n",
        "&= \\frac{1}{2}\\left(\\sum_{i=1}^n \\sum_{j=1}^n \\sum_{f=1}^k v_{i,f} v_{j,f} x_{i} x_{j}  -  \\sum_{i=1}^n \\sum_{f=1}^k v_{i,f} v_{i,f} x_{i} x_{i} \\right) \\\\\n",
        "&= \\frac{1}{2} \\sum_{f=1}^{k} \\left( \\left(\\sum_{i=1}^n v_{i,f}x_{i} \\right) \\left( \\sum_{j=1}^n v_{j,f}x_{j} \\right) - \\sum_{i=1}^{n} v_{i,f}^2 x_{i}^2 \\right) \\\\\n",
        "&= \\frac{1}{2} \\sum_{f=1}^{k} \\left( \\left( \\sum_{i}^{n} v_{i,f}x_{i} \\right)^2  - \\sum_{i=1}^{n} v_{i,f}^2 x_{i}^2 \\right)\n",
        "\\end{align}\n",
        "\n",
        "Note, summing over different pairs is the same as summing over all pairs minus the self-interactions (divided by two). This is the reason why the value 1/2 is introduced from the beginning of the derivation.\n",
        "\n",
        "This reformulated equation has a linear complexity in both $k$ and $n$, i.e. its computation is in $O(kn)$, substituting this new equation into the existing factorization machine formula, we end up with:\n",
        "\n",
        "\\begin{align}\n",
        "\\hat{y}(\\textbf{x}) = w_{0} + \\sum_{i=1}^{n} w_{i} x_{i} + \\frac{1}{2} \\sum_{f=1}^{k} \\left( \\left( \\sum_{i}^{n} v_{i,f}x_{i} \\right)^2  - \\sum_{i=1}^{n} v_{i,f}^2 x_{i}^2 \\right)\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkB7ed28DEce"
      },
      "source": [
        "In a machine learning setting, factorization machine can be applied to different supervised prediction tasks:\n",
        "\n",
        "- **Regression:**, in this case $\\hat{y}(\\textbf{x})$ can be used directly by minimizing the mean squared error between the model prediction and target value, e.g. $\\frac{1}{N}\\sum^{N}\\big(y - \\hat{y}(\\textbf{x})\\big)^2$\n",
        "- **Classification:**, if we were to use it in a binary classification setting, we could then minimize the log loss, $\\ln \\big(e^{-y \\cdot \\hat{y}(\\textbf{x})} + 1 \\big)$, where $\\sigma$ is the sigmoid/logistic function and $y \\in {-1, 1}$.\n",
        "\n",
        "To train factorization machine, we can use a gradient descent based optimization techniques, the parameters to be learned are $(w_0, \\mathbf{w},$ and $\\mathbf{V}$).\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial}{\\partial\\theta}\\hat{y}(\\textbf{x}) =\n",
        "\\begin{cases}\n",
        "1,  & \\text{if $\\theta$ is $w_0$} \\\\\n",
        "x_i, & \\text{if $\\theta$ is $w_i$} \\\\\n",
        "x_i\\sum_{j=1}^{n} v_{j,f}x_j - v_{i,f}x_{i}^2 & \\text{if $\\theta$ is $v_{i,f}$}\n",
        "\\end{cases}\n",
        "\\end{align}\n",
        "\n",
        "- Notice that $\\sum_{j=1}^n v_{j, f} x_j$ does not depend on $i$, thus it can be computed independently.\n",
        "- The last formula above, can also be written as $x_i(\\sum_{j=1}^{n} v_{j,f}x_j - v_{i,f}x_{i})$.\n",
        "- In practice, we would throw in some L2 regularization to prevent overfitting.\n",
        "\n",
        "As the next section contains implementation of the algorithm from scratch, the gradient of the log loss is also provided here for completeness. The predicted value $\\hat{y}(\\textbf{x})$ is replaced with $x$ for making the notation cleaner.\n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{d}{dx}\\left[ \\ln \\big(e^{-yx} + 1 \\big) \\right] \n",
        "&= \\frac{1}{e^{-yx} + 1} \\cdot  \\frac{d}{dx}\\left[e^{-yx} + 1 \\right] \\\\\n",
        "&= \\frac{\\frac{d}{dx}\\left[e^{-yx} \\right] + \\frac{d}{dx}\\left[1 \\right]}{e^{-yx} + 1} \\\\\n",
        "&= \\frac{e^{-yx} \\cdot \\frac{d}{dx}\\left[-yx \\right] + 0}{e^{-yx} + 1} \\\\\n",
        "&= \\frac{e^{-yx} \\cdot -y}{e^{-yx} + 1} \\\\\n",
        "&= -\\frac{ye^{-yx}}{e^{-yx} + 1} \\\\\n",
        "&= -\\frac{y}{e^{yx} + 1}\n",
        "\\end{align}\n",
        "\n",
        "---\n",
        "\n",
        "**Advantages:** We'll now wrap up the theoretical section of factorization machine, with some of its advantages:\n",
        "\n",
        "- We can observe from the model equation that it can be computed in linear time.\n",
        "- By leveraging ideas from matrix factorization, we can estimate higher order interaction effects even under very sparse data.\n",
        "- Compared to traditional matrix factorization methods, which is restricted to modeling a user-item matrix, we can leverage other user or item specific features making factorization machine more flexible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "te-Qa_04DEcf"
      },
      "source": [
        "## Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsFZtKHWDEcf"
      },
      "source": [
        "For the implementation of factorization machine, we'll use a for loop based code as I personally find it easier to comprehend for the gradient update section. There are different ways to speed up for loop based code in Python, such as using [Cython or Numba](http://nbviewer.jupyter.org/github/ethen8181/machine-learning/blob/master/python/cython/cython.ipynb), here we'll be using Numba."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7mJH3-ADEcg",
        "outputId": "e5394ffe-d283-4a49-b9b7-e10160d1de80"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<4179x3508 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 51261 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# using the example spam dataset\n",
        "# read it in, extract the input and output columns\n",
        "\n",
        "label_col = 'label_num'\n",
        "sms = pd.read_table('sms.tsv', header = None, names = ['label', 'message'])\n",
        "#print(sms)\n",
        "sms[label_col] = sms['label'].map({'ham': 0, 'spam': 1})\n",
        "X = sms['message']\n",
        "y = sms[label_col].values\n",
        "#print(X)\n",
        "# split X and y into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size = 0.25, random_state = 1)\n",
        "\n",
        "# convert both sets' text column to document-term matrix;\n",
        "# ideally, we would want to perform some preprocessing on\n",
        "# our text data, but let's be lazy here as that's not\n",
        "# the goal of this documentation\n",
        "tfidf = TfidfVectorizer(min_df = 2, max_df = 0.5)\n",
        "#print(tfidf.get_feature_names_out())\n",
        "#print(tfidf.to_array())\n",
        "X_train_dtm = tfidf.fit_transform(X_train)\n",
        "X_test_dtm = tfidf.transform(X_test)\n",
        "X_train_dtm\n",
        "#print(X_train_dtm.toarray())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "70vdF2a9DEcg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numba import njit\n",
        "from tqdm import trange\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "\n",
        "\n",
        "class FactorizationMachineClassifier(BaseEstimator, ClassifierMixin):\n",
        "    \"\"\"\n",
        "    Factorization Machine [1]_ using Stochastic Gradient Descent.\n",
        "    For binary classification only.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_iter : int, default 10\n",
        "        Number of iterations to train the algorithm.\n",
        "\n",
        "    n_factors : int, default 10\n",
        "        Number/dimension of features' latent factors.\n",
        "\n",
        "    learning_rate : float, default 0.1\n",
        "        Learning rate for the gradient descent optimizer.\n",
        "\n",
        "    reg_coef : float, default 0.01\n",
        "        Regularization strength for weights/coefficients.\n",
        "\n",
        "    reg_factors : float, default 0.01\n",
        "        Regularization strength for features' latent factors.\n",
        "\n",
        "    random_state : int, default 1234\n",
        "        Seed for the randomly initialized features latent factors\n",
        "\n",
        "    verbose : bool, default True\n",
        "        Whether to print progress bar while training.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    intercept_ : double\n",
        "        Intercept term, w0 based on the original notations.\n",
        "\n",
        "    coef_ : 1d ndarray, shape [n_features,]\n",
        "        Coefficients, w based on the original notations.\n",
        "\n",
        "    feature_factors_ : 2d ndarray, shape [n_factors, n_features]\n",
        "        Latent factors for all features. v based on the original\n",
        "        notations. The learned factors can be viewed as the\n",
        "        embeddings for each features. If a pair of features tends\n",
        "        to co-occur often, then their embeddings should be\n",
        "        close/similar (in terms of cosine similarity) to each other.\n",
        "\n",
        "    history_ : list\n",
        "        Loss function's history at each iteration, useful\n",
        "        for evaluating whether the algorithm converged or not.\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    .. [1] `S. Rendle Factorization Machines (2010)\n",
        "            <http://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf>`_ \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_iter = 10, n_factors = 10,\n",
        "                 learning_rate = 0.1, reg_coef = 0.01,\n",
        "                 reg_factors = 0.01, random_state = 1234, verbose = False):\n",
        "        self.n_iter = n_iter\n",
        "        self.verbose = verbose\n",
        "        self.reg_coef = reg_coef\n",
        "        self.n_factors = n_factors\n",
        "        self.reg_factors = reg_factors\n",
        "        self.random_state = random_state\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit the model to the input data and label.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : scipy sparse csr_matrix, shape [n_samples, n_features]\n",
        "            Data in sparse matrix format.\n",
        "\n",
        "        y : 1d ndarray, shape [n_samples,]\n",
        "            Training data's corresponding label.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        self\n",
        "        \"\"\"\n",
        "\n",
        "        n_samples, n_features = X.shape\n",
        "        self.coef_ = np.zeros(n_features)\n",
        "        self.intercept_ = 0.0\n",
        "\n",
        "        # the factors are often initialized with a mean of 0 and standard deviation\n",
        "        # of 1 / sqrt(number of latent factor specified)\n",
        "        np.random.seed(self.random_state)\n",
        "        self.feature_factors_ = np.random.normal(\n",
        "            scale = 1 / np.sqrt(self.n_factors), size = (self.n_factors, n_features))\n",
        "        \n",
        "        # the gradient is implemented in a way that requires\n",
        "        # the negative class to be labeled as -1 instead of 0\n",
        "        y = y.copy().astype(np.int32)\n",
        "        y[y == 0] = -1\n",
        "\n",
        "        loop = range(self.n_iter)\n",
        "        if self.verbose:\n",
        "            loop = trange(self.n_iter)\n",
        "\n",
        "        self.history_ = []\n",
        "        for _ in loop:\n",
        "            loss = _sgd_update(X.data, X.indptr, X.indices,\n",
        "                               y, n_samples, n_features,\n",
        "                               self.intercept_, self.coef_,\n",
        "                               self.feature_factors_, self.n_factors,\n",
        "                               self.learning_rate, self.reg_coef, self.reg_factors)\n",
        "            self.history_.append(loss)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Probability estimates. The returned estimates for\n",
        "        all classes are ordered by the label of classes.\n",
        "\n",
        "        Paramters\n",
        "        ---------\n",
        "        X : scipy sparse csr_matrix, shape [n_samples, n_features]\n",
        "            Data in sparse matrix format.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        proba : 2d ndarray, shape [n_samples, n_classes]\n",
        "            The probability of the sample for each class in the model.\n",
        "        \"\"\"\n",
        "        pred = self._predict(X)\n",
        "        pred_proba = 1.0 / (1.0 + np.exp(-pred))\n",
        "        proba = np.vstack((1 - pred_proba, pred_proba)).T\n",
        "        return proba\n",
        "\n",
        "    def _predict(self, X):\n",
        "        \"\"\"Similar to _predict_instance but vectorized for all samples\"\"\"\n",
        "        linear_output = X * self.coef_\n",
        "        v = self.feature_factors_.T\n",
        "        term = (X * v) ** 2 - (X.power(2) * (v ** 2))\n",
        "        factor_output = 0.5 * np.sum(term, axis = 1)\n",
        "        return self.intercept_ + linear_output + factor_output\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict class labels for samples in X.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : scipy sparse csr_matrix, shape [n_samples, n_features]\n",
        "            Data in sparse matrix format.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Predicted class label per sample.\n",
        "        \"\"\"\n",
        "        pred_proba = self.predict_proba(X)[:, 1]\n",
        "        return pred_proba.round().astype(np.int)\n",
        "\n",
        "\n",
        "@njit\n",
        "def _sgd_update(data, indptr, indices, y, n_samples, n_features,\n",
        "                w0, w, v, n_factors, learning_rate, reg_w, reg_v):\n",
        "    \"\"\"\n",
        "    Compute the loss of the current iteration and update\n",
        "    gradients accordingly.\n",
        "    \"\"\"\n",
        "    loss = 0.0\n",
        "    for i in range(n_samples):\n",
        "        pred, summed = _predict_instance(data, indptr, indices, w0, w, v, n_factors, i)\n",
        "        \n",
        "        # calculate loss and its gradient\n",
        "        loss += _log_loss(pred, y[i])\n",
        "        loss_gradient = -y[i] / (np.exp(y[i] * pred) + 1.0)\n",
        "    \n",
        "        # update bias/intercept term\n",
        "        w0 -= learning_rate * loss_gradient\n",
        "\n",
        "        # update weight\n",
        "        for index in range(indptr[i], indptr[i + 1]):\n",
        "            feature = indices[index]\n",
        "            w[feature] -= learning_rate * (loss_gradient * data[index] + 2 * reg_w * w[feature])\n",
        "\n",
        "        # update factor\n",
        "        for factor in range(n_factors):\n",
        "            for index in range(indptr[i], indptr[i + 1]):\n",
        "                feature = indices[index]\n",
        "                term = summed[factor] - v[factor, feature] * data[index]\n",
        "                v_gradient = loss_gradient * data[index] * term\n",
        "                v[factor, feature] -= learning_rate * (v_gradient + 2 * reg_v * v[factor, feature])\n",
        "    \n",
        "    loss /= n_samples\n",
        "    return loss\n",
        "\n",
        "\n",
        "@njit\n",
        "def _predict_instance(data, indptr, indices, w0, w, v, n_factors, i):\n",
        "    \"\"\"predicting a single instance\"\"\"\n",
        "    summed = np.zeros(n_factors)\n",
        "    summed_squared = np.zeros(n_factors)\n",
        "\n",
        "    # linear output w * x\n",
        "    pred = w0\n",
        "    for index in range(indptr[i], indptr[i + 1]):\n",
        "        feature = indices[index]\n",
        "        pred += w[feature] * data[index]\n",
        "\n",
        "    # factor output\n",
        "    for factor in range(n_factors):\n",
        "        for index in range(indptr[i], indptr[i + 1]):\n",
        "            feature = indices[index]\n",
        "            term = v[factor, feature] * data[index]\n",
        "            summed[factor] += term\n",
        "            summed_squared[factor] += term * term\n",
        "\n",
        "        pred += 0.5 * (summed[factor] * summed[factor] - summed_squared[factor])\n",
        "    \n",
        "    # summed is the independent term that can be re-used\n",
        "    # during the gradient update stage\n",
        "    return pred, summed\n",
        "\n",
        "\n",
        "@njit\n",
        "def _log_loss(pred, y):\n",
        "    \"\"\"\n",
        "    negative log likelihood of the\n",
        "    current prediction and label, y.\n",
        "    \"\"\"\n",
        "    return np.log(np.exp(-pred * y) + 1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "EvY70rTGDEch",
        "outputId": "8ecdc3a5-e7be-4a19-9dd2-378c5a61ecba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FactorizationMachineClassifier(n_iter=30)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>FactorizationMachineClassifier(n_iter=30)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">FactorizationMachineClassifier</label><div class=\"sk-toggleable__content\"><pre>FactorizationMachineClassifier(n_iter=30)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "fm = FactorizationMachineClassifier(n_iter = 30, learning_rate = 0.1)\n",
        "fm.fit(X_train_dtm, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "2wnAzmFkDEch",
        "outputId": "6d24f852-522b-4d19-f6e1-cd728b6bcc40"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# change default style figure and font size\n",
        "plt.rcParams['figure.figsize'] = 8, 6\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "# one quick way to check that we've implemented\n",
        "# the gradient descent is to ensure that the loss\n",
        "# curve is steadily decreasing\n",
        "plt.plot(fm.history_)\n",
        "plt.title('Loss Curve Per Iteration')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZaccGhiDEch",
        "outputId": "4ebcdc2f-6f75-45fc-cfba-78209f091800"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "auc 0.9973867907642743\n"
          ]
        }
      ],
      "source": [
        "# predict on the test set and output the auc score\n",
        "y_pred_prob = fm.predict_proba(X_test_dtm)[:, 1]\n",
        "auc = roc_auc_score(y_test, y_pred_prob)\n",
        "print('auc', auc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNrF4-SNDEci",
        "outputId": "c5df9f72-7967-4064-ddcc-b0e928ee18c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "auc 0.9950017898693396\n"
          ]
        }
      ],
      "source": [
        "# we can compare it with a logistic regression,\n",
        "logreg = LogisticRegression()\n",
        "logreg.fit(X_train_dtm, y_train)\n",
        "y_pred_prob = logreg.predict_proba(X_test_dtm)[:, 1]\n",
        "auc = roc_auc_score(y_test, y_pred_prob)\n",
        "print('auc', auc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9kJ0tkpDEci"
      },
      "source": [
        "There are various open-sourced implementations floating around the web, here are the links to some of them:\n",
        "\n",
        "- https://github.com/ibayer/fastFM\n",
        "- https://github.com/srendle/libfm\n",
        "- https://github.com/aksnzhy/xlearn\n",
        "- https://github.com/scikit-learn-contrib/polylearn\n",
        "\n",
        "I personally haven't tested which one is more efficient, feel free to grab one of them as see if it helps solve your problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "aNK0smQWDEci"
      },
      "source": [
        "# Reference\n",
        "\n",
        "- [Blog: Factorization Machines](http://www.jefkine.com/recsys/2017/03/27/factorization-machines/)\n",
        "- [Blog: Deep Understanding of FFM Principles and Practices (Chinese)](https://tech.meituan.com/deep-understanding-of-ffm-principles-and-practices.html)\n",
        "- [Quora: What are the drawbacks of Factorization Machines?](https://www.quora.com/What-are-the-drawbacks-of-Factorization-Machines)\n",
        "- [Paper: S. Rendle Factorization Machines (2010)](https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {
        "height": "12px",
        "width": "252px"
      },
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {},
      "toc_section_display": "block",
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "vscode": {
      "interpreter": {
        "hash": "c261aea317cc0286b3b3261fbba9abdec21eaa57589985bb7a274bf54d6cc0a7"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}